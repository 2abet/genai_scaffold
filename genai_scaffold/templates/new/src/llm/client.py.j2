"""LLM client implementation for {{ llm_provider }}."""

from typing import List, Optional, Dict, Any
{% if use_openai -%}
from openai import OpenAI
{% endif -%}
{% if use_anthropic -%}
from anthropic import Anthropic
{% endif -%}
{% if use_azure -%}
from openai import AzureOpenAI
{% endif -%}
{% if use_ollama -%}
import ollama
{% endif %}

from ..config import Config
{% if enable_observability -%}
from ..observability import trace_llm_call
{% endif %}


class LLMClient:
    """Client for interacting with {{ llm_provider }} LLM."""
    
    def __init__(self):
        """Initialize the LLM client."""
{% if use_openai -%}
        self.client = OpenAI(api_key=Config.OPENAI_API_KEY)
        self.model = Config.OPENAI_MODEL
{% endif -%}
{% if use_anthropic -%}
        self.client = Anthropic(api_key=Config.ANTHROPIC_API_KEY)
        self.model = Config.ANTHROPIC_MODEL
{% endif -%}
{% if use_azure -%}
        self.client = AzureOpenAI(
            api_key=Config.AZURE_OPENAI_API_KEY,
            api_version=Config.AZURE_OPENAI_API_VERSION,
            azure_endpoint=Config.AZURE_OPENAI_ENDPOINT
        )
        self.model = Config.AZURE_OPENAI_DEPLOYMENT
{% endif -%}
{% if use_ollama -%}
        self.base_url = Config.OLLAMA_BASE_URL
        self.model = Config.OLLAMA_MODEL
{% endif %}
    
{% if enable_observability -%}
    @trace_llm_call
{% endif -%}
    def generate(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> str:
        """Generate a response from the LLM.
        
        Args:
            prompt: The input prompt
            temperature: Sampling temperature (0.0 to 1.0)
            max_tokens: Maximum tokens to generate
            **kwargs: Additional model-specific parameters
            
        Returns:
            Generated text response
        """
{% if use_openai or use_azure -%}
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs
        )
        return response.choices[0].message.content
{% endif -%}
{% if use_anthropic -%}
        response = self.client.messages.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature,
            max_tokens=max_tokens or 1024,
            **kwargs
        )
        return response.content[0].text
{% endif -%}
{% if use_ollama -%}
        response = ollama.chat(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            options={
                "temperature": temperature,
                **({"num_predict": max_tokens} if max_tokens else {}),
                **kwargs
            }
        )
        return response['message']['content']
{% endif %}
    
{% if enable_observability -%}
    @trace_llm_call
{% endif -%}
    def generate_with_context(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> str:
        """Generate a response with conversation context.
        
        Args:
            messages: List of message dicts with 'role' and 'content'
            temperature: Sampling temperature
            max_tokens: Maximum tokens to generate
            **kwargs: Additional parameters
            
        Returns:
            Generated response
        """
{% if use_openai or use_azure -%}
        response = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs
        )
        return response.choices[0].message.content
{% endif -%}
{% if use_anthropic -%}
        response = self.client.messages.create(
            model=self.model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens or 1024,
            **kwargs
        )
        return response.content[0].text
{% endif -%}
{% if use_ollama -%}
        response = ollama.chat(
            model=self.model,
            messages=messages,
            options={
                "temperature": temperature,
                **({"num_predict": max_tokens} if max_tokens else {}),
                **kwargs
            }
        )
        return response['message']['content']
{% endif %}
    
    def embed(self, text: str) -> List[float]:
        """Generate embeddings for text.
        
        Args:
            text: Text to embed
            
        Returns:
            List of embedding values
        """
{% if use_openai or use_azure -%}
        response = self.client.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return response.data[0].embedding
{% endif -%}
{% if use_anthropic -%}
        # Anthropic doesn't provide embeddings, use a different service
        raise NotImplementedError("Anthropic doesn't provide embeddings. Use OpenAI or another service.")
{% endif -%}
{% if use_ollama -%}
        response = ollama.embeddings(
            model=self.model,
            prompt=text
        )
        return response['embedding']
{% endif %}
