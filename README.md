# genai-scaffold

**genai-scaffold** is an interactive Python CLI tool that bootstraps production-ready Generative AI project structures with customizable tech stacks, best practices, and modular organization.

![PyPI](https://img.shields.io/pypi/v/genai-scaffold)
![License](https://img.shields.io/pypi/l/genai-scaffold)

---

## âœ¨ Features

- ğŸ¯ **Interactive CLI**: Choose your tech stack interactively with a beautiful terminal UI
- ğŸ§  **Multiple LLM Providers**: OpenAI, Anthropic (Claude), Azure OpenAI, or Ollama (local)
- ğŸ”§ **Orchestration Frameworks**: LangChain, LlamaIndex, DSPy, or raw Python
- ğŸ’¾ **Vector Databases**: Pinecone, ChromaDB, Qdrant, or PostgreSQL with pgvector
- ğŸ¨ **UI Frameworks**: Streamlit, Gradio, FastAPI, or headless
- ğŸ“¦ **Dependency Management**: Poetry or pip (requirements.txt)
- ğŸ³ **Docker Support**: Automatic docker-compose configuration for local services
- ğŸ“Š **Observability**: Optional LangSmith or Weights & Biases integration
- ğŸ§ª **Testing**: Pre-configured pytest setup with example tests
- ğŸ“ **Prompt Management**: YAML-based prompt templates with versioning
- ğŸš€ **Production-Ready**: Makefile, .env management, logging, and best practices

---

## ğŸ“¦ Installation

You can install it via [PyPI](https://pypi.org/project/genai-scaffold):

```bash
pip install genai-scaffold
```

Or using `pipx`:

```bash
pipx install genai-scaffold
```

---

## ğŸš€ Usage

### Interactive Mode (Recommended)

Simply run the create command with the `--interactive` flag or without any arguments:

```bash
genai-scaffold create --interactive
```

Or just:

```bash
genai-scaffold create
```

This will launch an interactive wizard that guides you through selecting:
- Project name
- LLM provider (OpenAI, Anthropic, Azure, Ollama)
- Orchestration framework (LangChain, LlamaIndex, DSPy, None)
- Vector database (ChromaDB, Pinecone, Qdrant, pgvector)
- UI framework (Streamlit, Gradio, FastAPI, None)
- Dependency manager (pip or Poetry)
- Docker configuration
- Observability tools

### Command-Line Mode

For automation or quick scaffolding, specify all options via flags:

```bash
genai-scaffold create my-rag-app \
  --provider openai \
  --orchestrator langchain \
  --vector-db chromadb \
  --ui streamlit \
  --deps pip
```

### Example: Create a RAG App with LangChain and Streamlit

```bash
genai-scaffold create my-chatbot \
  --provider anthropic \
  --orchestrator langchain \
  --vector-db pinecone \
  --ui streamlit
```

### Example: Create a DSPy App with Local Models

```bash
genai-scaffold create local-ai-app \
  --provider ollama \
  --orchestrator dspy \
  --vector-db chromadb \
  --ui gradio \
  --no-docker
```

### Generated Project Structure

```
my-rag-app/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ llm/              # LLM client implementation
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ client.py
â”‚   â”œâ”€â”€ prompts/          # Prompt templates and management
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ loader.py
â”‚   â”‚   â””â”€â”€ templates.yaml
â”‚   â”œâ”€â”€ utils/            # Utility functions (logging, etc.)
â”‚   â”œâ”€â”€ config.py         # Configuration management
â”‚   â”œâ”€â”€ vector_store.py   # Vector database interface
â”‚   â””â”€â”€ rag_pipeline.py   # RAG implementation (if orchestrator selected)
â”œâ”€â”€ tests/                # Pytest test suite
â”‚   â”œâ”€â”€ conftest.py
â”‚   â””â”€â”€ test_example.py
â”œâ”€â”€ data/                 # Data directories
â”‚   â”œâ”€â”€ cache/
â”‚   â”œâ”€â”€ outputs/
â”‚   â””â”€â”€ embeddings/
â”œâ”€â”€ notebooks/            # Jupyter notebooks (optional)
â”œâ”€â”€ app.py                # UI application (Streamlit/Gradio/FastAPI)
â”œâ”€â”€ docker-compose.yml    # Docker services (if enabled)
â”œâ”€â”€ Dockerfile            # Application container
â”œâ”€â”€ .env.example          # Environment variables template
â”œâ”€â”€ Makefile              # Common tasks (setup, test, run, etc.)
â”œâ”€â”€ requirements.txt      # Python dependencies (or pyproject.toml)
â”œâ”€â”€ pytest.ini            # Pytest configuration
â””â”€â”€ README.md             # Project documentation
```

---

## ğŸ¯ Quick Start with Generated Project

After scaffolding your project:

```bash
# 1. Navigate to your project
cd my-rag-app

# 2. Set up environment variables
cp .env.example .env
# Edit .env with your API keys

# 3. Install dependencies
make setup
# or: pip install -r requirements.txt

# 4. Start services with Docker (if enabled)
docker-compose up -d

# 5. Run the application
make run

# 6. Run tests
make test
```

---

## ğŸ› ï¸ Available Commands

### Create Command

```bash
genai-scaffold create [PROJECT_NAME] [OPTIONS]
```

**Options:**
- `--provider`: LLM provider (openai, anthropic, azure, ollama)
- `--orchestrator`: Framework (langchain, llamaindex, dspy, none)
- `--vector-db`: Vector database (pinecone, chromadb, qdrant, pgvector)
- `--ui`: UI framework (streamlit, gradio, fastapi, none)
- `--deps`: Dependency manager (pip, poetry)
- `--docker/--no-docker`: Enable/disable Docker configuration
- `--interactive, -i`: Use interactive mode

### Version Command

```bash
genai-scaffold version
```

---

## ğŸ”§ Tech Stack Options

### LLM Providers
- **OpenAI**: GPT-4, GPT-3.5, and embedding models
- **Anthropic**: Claude 3 (Opus, Sonnet, Haiku)
- **Azure OpenAI**: Enterprise-grade OpenAI models
- **Ollama**: Local models (Llama 2, Mistral, etc.)

### Orchestration Frameworks
- **LangChain**: Full-featured LLM framework with chains and agents
- **LlamaIndex**: Data framework for LLM applications
- **DSPy**: Declarative language model programming
- **None**: Raw Python with custom implementation

### Vector Databases
- **ChromaDB**: Easy-to-use, local-first vector store
- **Pinecone**: Managed vector database service
- **Qdrant**: High-performance vector search engine
- **pgvector**: PostgreSQL extension for vector operations

### UI Frameworks
- **Streamlit**: Fast way to build data apps
- **Gradio**: Quick ML model interfaces
- **FastAPI**: Modern, fast API framework
- **None**: Headless/CLI application

---

## ğŸ“š Generated Features

### Prompt Management

The generated projects include a sophisticated prompt management system:

```python
from src.prompts import load_prompt

# Load and format a prompt template
prompt = load_prompt("rag_query", context="...", question="...")
```

Prompts are stored in `src/prompts/templates.yaml` with versioning support.

### Configuration Management

Environment-based configuration with validation:

```python
from src.config import Config

# Access configuration
api_key = Config.OPENAI_API_KEY
model = Config.OPENAI_MODEL
```

### Logging

Pre-configured logging utilities:

```python
from src.utils import get_logger

logger = get_logger(__name__)
logger.info("Processing request...")
```

### Observability (Optional)

If enabled, automatic tracing with LangSmith or W&B:

```python
from src.observability import trace_llm_call

@trace_llm_call
def my_llm_function():
    # Automatically traced
    pass
```

---

## ğŸ§ª Testing

Generated projects include a complete test setup:

```bash
# Run tests
make test

# Run with coverage
make test-coverage

# Run specific test
pytest tests/test_example.py -v
```

---

## ğŸ³ Docker Support

When Docker is enabled, projects include:

- `docker-compose.yml` with service definitions
- `Dockerfile` for the application
- Automatic configuration for:
  - ChromaDB server (if selected)
  - Qdrant server (if selected)
  - PostgreSQL with pgvector (if selected)

Start all services:

```bash
docker-compose up -d
```

---

## ğŸ”„ Makefile Commands

Generated projects include a Makefile with common tasks:

```bash
make setup          # Install dependencies
make test           # Run tests
make test-coverage  # Run tests with coverage
make run            # Run the application
make format         # Format code (black, isort)
make lint           # Run linter (ruff)
make clean          # Clean build artifacts
make docker-up      # Start Docker services (if Docker enabled)
make docker-down    # Stop Docker services
```

---

## ğŸ¤ Contributing

Pull requests are welcome! For major changes, open an issue first to discuss what you'd like to change.

---

## ğŸ“„ License

This project is licensed under the MIT License.

---

## ğŸ™Œ Acknowledgements

Built for developers who want to quickly scaffold production-ready GenAI applications with best practices and flexibility.
